{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets with NLTK and Python\n",
    "\n",
    "###### By Eleanor Stribling, July 2018\n",
    "\n",
    "<img src=\"img/nasa_twitter.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enjoy Natural Language Processing like I do, you might have some ideas for analyses you could do using Twitter. What are the word patterns and hashtags that the tweeter uses?  How do these change over time?  What sites do they link to?  What words appear around others?  Who or what do they talk about?\n",
    "\n",
    "In [part 1](https://medium.com/agatha-codes/0-to-280-getting-twitter-timeline-data-for-your-awesome-nlp-project-ff41b941ed6), I used [NASA's Twitter feed](https://twitter.com/NASA) as an example. This time I'm going to do perhaps the most obvious analysis: using [Donald Trump's tweets](https://twitter.com/realdonaldtrump).  For full disclosure, I am as anti-Trump as they come, but I do this analysis the same way I would approach any project, and all of the code is publicly available so you can evaluate it for yourself.  Quantifying what he says was honestly a little depressing, but I hope that an example that is extreme and in the public consciousness in the United States and beyond will demonstrate the power of these techniques and how a straightforward analysis of his tweets betrays his worst characteristics.\n",
    "\n",
    "\n",
    "If you missed part 1, you can view it on [Github](https://github.com/eleanorstrib/twitter_timeline_analysis_1) and the accompanying [post](https://medium.com/agatha-codes/0-to-280-getting-twitter-timeline-data-for-your-awesome-nlp-project-ff41b941ed6) on my blog, [agatha.codes](https://medium.com/agatha-codes).\n",
    "\n",
    "Here's a summary of what we've completed in part 1 (‚úÖ) and what we will cover in this post (üÜï) ¬†:\n",
    "\n",
    "A. Obtain and clean the data from a Twitter user's timeline ‚úÖ\n",
    "\n",
    "B. Export into a csv file and a Python dictionary ‚úÖ\n",
    "\n",
    "C. Apply analysis tools from NLTK to get summary data and explore hypotheses about the user's tweets üÜï\n",
    "\n",
    "D. Use Python libraries to visualize the data you collect üÜï\n",
    "\n",
    "This tutorial can be found on [Github](https://github.com/eleanorstrib/twitter_timeline_analysis_2) and [agatha.codes](https://medium.com/p/5a2d387ce37d/edit).  Hope you find it useful for your own projects, and please let me know via the comments section on the blog post of any feedback or questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the steps from part 1 ‚úÖ\n",
    "In the cells below, I'm re-running some of the steps from the previous tutorial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import twitter\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for keys and tokens to access the Twitter API\n",
    "TWITTER_CONS_KEY = os.environ.get('T_CONS_')\n",
    "TWITTER_CONS_SEC = os.environ.get('T_CONS_SECRET')\n",
    "TWITTER_ACCESS_TOKEN = os.environ.get('T_ACCESS_')\n",
    "TWITTER_ACCESS_SEC = os.environ.get('T_ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object for querying the API using the python-twitter library\n",
    "t = twitter.Api(\n",
    "    consumer_key = TWITTER_CONS_KEY,\n",
    "    consumer_secret = TWITTER_CONS_SEC,\n",
    "    access_token_key = TWITTER_ACCESS_TOKEN, \n",
    "    access_token_secret = TWITTER_ACCESS_SEC,\n",
    "    tweet_mode='extended' # this ensures that we get the full text of the users' original tweets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the account you want to analyze\n",
    "screen_name = \"realdonaldtrump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the Twitter API, get 200 tweets\n",
    "first_200 = t.GetUserTimeline(screen_name=screen_name, count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get more tweets up to the rate limit\n",
    "def get_tweets(first_200, screen_name, last_id):\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(first_200)\n",
    "    for i in range(900):\n",
    "        new = t.GetUserTimeline(screen_name=screen_name, max_id=last_id-1)\n",
    "        if len(new) > 0:\n",
    "            all_tweets.extend(new)\n",
    "            last_id = new[-1].id\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "all_tweets = get_tweets(first_200, screen_name, first_200[-1].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a variable, all_tweets with a big, reverse chronological list of tweet objects with these attributes\n",
    "print([a for a in dir(all_tweets[0]) if '_' not in a and a[0].islower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out cleaning funtions\n",
    "def clean_hashtags(hashtags):\n",
    "    \"\"\"\n",
    "    Turns data with any number of hashtags like this - [Hashtag(Text='STEMonStation')] - to a list like this -\n",
    "    ['STEMonStation']\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    if len(hashtags) >= 1:\n",
    "        for i in range(len(hashtags)):\n",
    "            cleaned.append(hashtags[i].text)        \n",
    "    return cleaned\n",
    "\n",
    "def clean_urls(urls):\n",
    "    \"\"\"\n",
    "    Turns data with any number of expanded urls like this - \n",
    "    [URL(URL=https://t.co/sYCFHKxzBf, ExpandedURL=https://youtu.be/34bFgA3H3hQ)]- to a list like this - \n",
    "    [\"https://youtu.be/34bFgA3H3hQ\"]\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    if len(urls) >= 1:\n",
    "        for i in range(len(urls)):\n",
    "            cleaned.append(urls[i].expanded_url)\n",
    "    return(cleaned)\n",
    "        \n",
    "\n",
    "def clean_source(source):\n",
    "    \"\"\"\n",
    "    Turns data including the source and some html like this - \n",
    "    <a href=\"https://www.sprinklr.com\" rel=\"nofollow\">Sprinklr</a> - to a list like this -\n",
    "    ['Sprinklr']\n",
    "    \"\"\"\n",
    "    raw = lxml.html.document_fromstring(source)\n",
    "    return raw.cssselect('body')[0].text_content()\n",
    "\n",
    "\n",
    "def string_to_datetime(date_str):\n",
    "    \"\"\"\n",
    "    Turns a string including date and time like this - Sun Jul 01 21:06:07 +0000 2018 - to a Python datetime object\n",
    "    like this - datetime.datetime(2018, 7, 1, 21, 6, 7, tzinfo=datetime.timezone.utc)\n",
    "    \"\"\"\n",
    "    return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to create a dict\n",
    "def create_dict(tweets):\n",
    "    dict = {}\n",
    "    for item in tweets:\n",
    "        clean_source(item.source)\n",
    "        dict[item.id_str] = {\n",
    "            'full_text': item.full_text,\n",
    "            'hashtags': clean_hashtags(item.hashtags),\n",
    "            'urls': clean_urls(item.urls),\n",
    "            'created_at': string_to_datetime(item.created_at),\n",
    "            'favorite_count': item.favorite_count,\n",
    "            'retweet_count' : item.retweet_count,\n",
    "            'source': clean_source(item.source)\n",
    "        }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function and create our dictionary\n",
    "tweet_dict = create_dict(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "if len(all_tweets) == len(tweet_dict):\n",
    "    print(\"We have all of the tweets in the dictionary that we collected from the API!  {} total. ‚ò∫Ô∏è\".format(len(tweet_dict)))\n",
    "else:\n",
    "    print(\"Something went wrong - check the create_dict function to ensure no tweets were missed. üò¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we covered in the last tutorial.  Now let's get to the *really* fun part - analysis!\n",
    "\n",
    "That said, the basic analysis below will help you to explore the data and choose a hypothesis to dig into.  That I will demonstrate below.\n",
    "\n",
    "Let's get started with the new material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get some summary stats on the data set üìä\n",
    "\n",
    "Getting a high level understanding of what's in the data set is usually my first step. Let's start by getting a high level quantitative view of what we have before we dive into the text.\n",
    "\n",
    "First, let's import some libraries we'll need for this tutorial.  There's a mix of quantitative tools for this step and NLTK features for looking at the text in step 2.\n",
    "\n",
    "Note that this introduces some new stuff you'll need to install in your virtual environment.  You can add these one by one or run `pip intall -r requirements.txt` - the `requirements.txt` file in this repo has been updated from the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams, trigrams, Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from prettytable import PrettyTable\n",
    "from scipy import stats\n",
    "from string import punctuation as punc\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of what each entry looks like.  Remember that `tweet_dict` is a dictionary where we are storing all of our data, the keys are strings with the ID number of the tweet, and the value (printed below) is another dictionary with the specifics on that tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet_dict['1048668088059584512'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few fields here we can quantify to get a broad idea of what the characteristics of the data set are:\n",
    "- High level picture of the data set (`num_and_date` function)\n",
    "   - Number of tweets\n",
    "   - Dates of most recent and oldest tweets\n",
    "- Summary stats on the numeric fields (`metric_summary` function)\n",
    "   - Likes\n",
    "   - Retweets\n",
    "- Summary stats on the strings that are not directly contained in the tweet (`enum_analysis_pie` function)\n",
    "   - Sources\n",
    "   - URLs\n",
    "\n",
    "The reason I'm not looking at the content of the tweets yet, is I want to use some NLTK tools on those, and they require a bit more cleaning before we can do much there.\n",
    "\n",
    "Let's write some code to get us that information in the `tweet_summary` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_and_date(name, all_tweets, tweet_dict):\n",
    "    # first, let's get the total number of tweets\n",
    "    total_tweets = len(tweet_dict)\n",
    "    \n",
    "    # now the date range - these are formatted as datetime objects so I can subtract one from the other\n",
    "    oldest_tweet_id, newest_tweet_id = str(all_tweets[-1].id), str(all_tweets[0].id)\n",
    "    oldest_tweet_date, newest_tweet_date = tweet_dict[oldest_tweet_id]['created_at'], tweet_dict[newest_tweet_id]['created_at']\n",
    "    date_range = (newest_tweet_date-oldest_tweet_date).days\n",
    "    \n",
    "    # print the results\n",
    "    print(\"Here's a high level summary for our dataset for {}:\".format(name))\n",
    "    print()\n",
    "    print(\"There are {} tweets total.\".format(total_tweets))\n",
    "    print()\n",
    "    print(\"These tweets were posted in the {} days between {}/{}/{} and {}/{}/{}.\".format(date_range,\n",
    "                                                                              oldest_tweet_date.month,\n",
    "                                                                              oldest_tweet_date.day,\n",
    "                                                                              oldest_tweet_date.year, \n",
    "                                                                              newest_tweet_date.month,\n",
    "                                                                              newest_tweet_date.day,\n",
    "                                                                              newest_tweet_date.year,))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_and_date(screen_name, all_tweets, tweet_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done that, the `metric_summary` function below will do some calculations for us on favorites and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we will pull the data we need out of the tweet_dict variable, and create big lists of every\n",
    "# date, favorite and rt; we'll use all_tweets because that's an ordered list and the values will align in our lists\n",
    "# while dictionaries are ordered in python 3*, doing it this way just in case you're using an older version ‚ò∫Ô∏è\n",
    "dates = [string_to_datetime(t.created_at) for t in all_tweets]\n",
    "f_arr = [t.favorite_count for t in all_tweets]\n",
    "r_arr = [t.retweet_count for t in all_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_summary(name, metric, arr, color, dates, td):\n",
    "    # first we're going to pull some summary stats for the account using the \"describe\" function from SciPy\n",
    "    data_sum = stats.describe(arr)\n",
    "    avg = int(data_sum.mean)\n",
    "    most = data_sum.minmax[1]\n",
    "    least = data_sum.minmax[0]\n",
    "    # here we're searching for the tweet with the biggest quantity of the metric we're looking at by getting the id\n",
    "    # and searching tweet_dict\n",
    "    top = [k for k, v in td.items() if v[metric+'_count'] == most]\n",
    "    \n",
    "    # print out the results\n",
    "    print(\"Let's look at {}s for the {} dataset\".format(metric, name))\n",
    "    print(\"The average {} tweet got {} {}s.\".format(name, avg, metric))\n",
    "    print(\"The lowest number of {}s a tweet got was {}.\".format(metric, least))\n",
    "    if len(top) >= 1:\n",
    "        top_tweet_id = top[0]\n",
    "        url = \"https://twitter.com/{}/status/{}\".format(name, top_tweet_id)\n",
    "        print(\"The most {}ed tweet in the dataset got {} favorites.  Here's the link: {}\".format(metric,most,url) )\n",
    "    \n",
    "    # plot the timeseries distribution to look for trends and get a visual\n",
    "    plt.plot_date(x=dates, y=arr, fmt=\"r-\", color=color)\n",
    "    plt.title(\"Timeseries plot of {}s for {}\".format(metric, name))\n",
    "    plt.ylabel(metric+\"s\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call metric_summary twice, once for favorites, and once for retweets\n",
    "metric_summary(screen_name, \"favorite\", f_arr, \"orange\", dates, tweet_dict)\n",
    "metric_summary(screen_name, \"retweet\", r_arr, \"blue\", dates, tweet_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example üîç\n",
    "\n",
    "You might be running this workbook on data other than the NASA Twitter account, but there were a lot of variables flying around in the functions above, so let's pause and show an example of what you should be seeing (more or less) and things you can draw from this data.\n",
    "\n",
    "The example below is an analysis of the NASA data set at the time of this workbook's writing, in October 2018.\n",
    "\n",
    "<img src=\"img/nasa_favert.png\"/>\n",
    "\n",
    "From this analysis, I can see that the most favorited and retweeted tweet has the same url, and it was about the death of Stephen Hawking.\n",
    "\n",
    "<img src=\"img/nasa_hawkingtweet.png\" align=\"left\" height=\"300\"/>\n",
    "\n",
    "I can also see from the charts and by the averages that this tweet was a serious outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've covered the high level stats for the data set and some quantitative measures for numerical fields, favorites and retweets, so the last part of this step is examining the source and url fields.\n",
    "\n",
    "Recall that we cleaned these two fields above and in the last tutorial to contain lists of sources and expanded urls.  These aren't quite enumerated fields, but they are likely close enough that we can take a quantitative approach to them.\n",
    "\n",
    "First, let's make lists of these as we did with favorites and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "s_arr = [v['source'] for k, v in tweet_dict.items()]\n",
    "print(\"The sources list looks like this: \", s_arr[0:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls\n",
    "u_arr = [v['urls'] for k, v in tweet_dict.items()]\n",
    "print(\"The urls list looks like this: \", u_arr[0:5])\n",
    "print()\n",
    "# for urls, we need another step, because unlike every other field we've used so far, there can be\n",
    "# more than one value in a list of values, and it's likely that each one is a link to a different page\n",
    "\n",
    "# to summarize these, we're going to use another list comprehension to flatten the list and use the urlparse method to \n",
    "# retain only the domain of the link (e.g. https://go.nasa.gov/2KF6Cnu and https://go.nasa.gov/7fh90y\n",
    "# should both count as go.nasa.gov)\n",
    "u_arr_flat = [urlparse(item).netloc for s in u_arr for item in s]\n",
    "\n",
    "# print the results to see what the data looks like now\n",
    "print(\"The cleaned url list looks like this: \", u_arr_flat[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_analysis_pie(name, metric, arr, top_num):\n",
    "    \n",
    "    # we use a Counter object to summarize our lists of values\n",
    "    counts = Counter(arr).most_common(top_num) # the function will take the most popular n values for the charts\n",
    "    \n",
    "    # counts is a list of tuples (e.g. ('Source', #))\n",
    "    labels = [c[0] for c in counts]\n",
    "    sizes = [c[1] for c in counts]\n",
    "    \n",
    "    # now let's plot it!\n",
    "    fig1, ax1 = plt.subplots(figsize=(7,7), subplot_kw=dict(aspect=\"equal\"))\n",
    "    ax1.pie(sizes, labels=labels, autopct='%1.1f%%',startangle=90)\n",
    "    ax1.axis('equal')  # equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title(\"Top {} {}s for {}'s tweets:\".format(top_num, metric, name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to display the pie charts for the distribution of the top 5 results for each data type\n",
    "enum_analysis_pie(screen_name, 'source', s_arr, 5)\n",
    "enum_analysis_pie(screen_name, 'url', u_arr_flat, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the NASA Twitter feed at the time of this writing (July 2018) here are the results I got and some analysis:\n",
    "\n",
    "For the sources, most of the NASA tweets are coming from Sprinklr, a social media management platform, or the Twitter Web Client.  One hypothesis there might be that they have a professional social media management team that's using tools that are somewhat scalable rather than apps on a mobile device.\n",
    "\n",
    "<img src=\"img/nasa_sourcepie.png\" width=\"50%\"/>\n",
    "\n",
    "For URLs, over 75% of the links shared are on the NASA site (nasa.gov with or without a subdomain - go.nasa.gov urls are shortened versions of pages on the nasa.gov site), suggesting that the Twitter feed's goal might be to draw traffic to the organization's website and it's own content over content elsewhere or by other creators.\n",
    "\n",
    "<img src=\"img/nasa_urlpie.png\" width=\"50%\"/>\n",
    "\n",
    "This is a good place to pause and consider what you've learned and your hypotheses so far.  From the NASA Twitter dataset that I worked with, I have the following information and hypotheses:\n",
    "\n",
    "- We have about 3,200 tweets in the dataset, covering about 8.5 months' worth of posts\n",
    "- An average tweet gets ~2,300 likes and ~900 retweets\n",
    "- The most popular tweet (in terms of both favorites and retweets) in our data set was about the death of Stephen Hawking\n",
    "\n",
    "Now let's take a look at the meat of the tweets: the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze the text of the tweets üìù\n",
    "\n",
    "This is where we get to use NLTK and hone in on some themes within the content of the tweets.\n",
    "\n",
    "Before we go much further, we'll have to do - you guessed it - a little more cleaning.  Specifically, because we're going to use some methods on the data that require removing punctuation and very common words (or stopwords), we will need to remove those.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = list(set(stopwords.words('english')))\n",
    "punc_list = [char for char in punc]\n",
    "special = ['//', 'http', 'https', '‚Äô', 'amp', 'rt']\n",
    "custom_cleaning = stopw + punc_list + special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_clean_add(tweet_dict, custom_cleaning):\n",
    "    big_word_list= []\n",
    "    hashtag_list = []\n",
    "    for k, v in tweet_dict.items():\n",
    "        full_text = word_tokenize(tweet_dict[k]['full_text'].lower())\n",
    "        hashtags = [h.lower() for h in tweet_dict[k]['hashtags']]\n",
    "        tweet_dict[k]['clean_text'] = [word for word in full_text if word not in custom_cleaning and 't.co' not in word and word not in hashtags]\n",
    "        big_word_list.extend(tweet_dict[k]['clean_text'])\n",
    "        hashtag_list.extend(tweet_dict[k]['hashtags'])\n",
    "    return (tweet_dict, big_word_list, hashtag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict, big_word_list, hashtag_list = tokenize_clean_add(tweet_dict, custom_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_word_list[:10])\n",
    "print(hashtag_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data - note that the times and numbers are slightly different from the first tutorial\n",
    "print(\"There are %d tweets stored in a list as the all_tweets variable.\" % len(all_tweets))\n",
    "print(\"The most recent tweet in our collection was sent %s and the oldest tweet was sent %s.\" % (\n",
    "                                                                            all_tweets[0].created_at, \n",
    "                                                                            all_tweets[-1].created_at)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "#generate the word cloud with parameters\n",
    "\n",
    "print(\"Here's a wordcloud of all of the text of %s's tweets.\" % screen_name)\n",
    "wc = WordCloud(background_color=\"white\", \n",
    "               max_words=50, \n",
    "               min_font_size =5, \n",
    "               max_font_size=50, \n",
    "               relative_scaling = 0.6, \n",
    "               normalize_plurals= True,\n",
    "               collocations=False)\n",
    "fig_sz = (20,20)\n",
    "\n",
    "wc.generate(' '.join(big_word_list))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is a wordcloud of all of the hashtags used in %s's tweets.\" % screen_name)\n",
    "wc.generate(' '.join(hashtag_list))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = Text(big_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.similar('hillary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.similar('russia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_grams = list(bigrams(big_word_list))\n",
    "bg_count = Counter(b_grams)\n",
    "print(bg_count.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
